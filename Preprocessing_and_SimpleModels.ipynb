{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/arthur/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/arthur/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/arthur/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import spacy\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# for NLTK and spaCy preprocessing\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read json file into dataframe\n",
    "df = pd.read_json('/Users/arthur/My Drive/Education/Master/Copenhagen Business School/2. Semester/Natural Language Processing and Text Analytics/Assignments/Final Assignment/News_Category_Dataset_v3.json', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting the Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(209527, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>headline</th>\n",
       "      <th>category</th>\n",
       "      <th>short_description</th>\n",
       "      <th>authors</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.huffpost.com/entry/covid-boosters-...</td>\n",
       "      <td>Over 4 Million Americans Roll Up Sleeves For O...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>Health experts said it is too early to predict...</td>\n",
       "      <td>Carla K. Johnson, AP</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffpost.com/entry/american-airlin...</td>\n",
       "      <td>American Airlines Flyer Charged, Banned For Li...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>He was subdued by passengers and crew when he ...</td>\n",
       "      <td>Mary Papenfuss</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.huffpost.com/entry/funniest-tweets...</td>\n",
       "      <td>23 Of The Funniest Tweets About Cats And Dogs ...</td>\n",
       "      <td>COMEDY</td>\n",
       "      <td>\"Until you have a dog you don't understand wha...</td>\n",
       "      <td>Elyse Wanshel</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.huffpost.com/entry/funniest-parent...</td>\n",
       "      <td>The Funniest Tweets From Parents This Week (Se...</td>\n",
       "      <td>PARENTING</td>\n",
       "      <td>\"Accidentally put grown-up toothpaste on my to...</td>\n",
       "      <td>Caroline Bologna</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.huffpost.com/entry/amy-cooper-lose...</td>\n",
       "      <td>Woman Who Called Cops On Black Bird-Watcher Lo...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>Amy Cooper accused investment firm Franklin Te...</td>\n",
       "      <td>Nina Golgowski</td>\n",
       "      <td>2022-09-22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link  \\\n",
       "0  https://www.huffpost.com/entry/covid-boosters-...   \n",
       "1  https://www.huffpost.com/entry/american-airlin...   \n",
       "2  https://www.huffpost.com/entry/funniest-tweets...   \n",
       "3  https://www.huffpost.com/entry/funniest-parent...   \n",
       "4  https://www.huffpost.com/entry/amy-cooper-lose...   \n",
       "\n",
       "                                            headline   category  \\\n",
       "0  Over 4 Million Americans Roll Up Sleeves For O...  U.S. NEWS   \n",
       "1  American Airlines Flyer Charged, Banned For Li...  U.S. NEWS   \n",
       "2  23 Of The Funniest Tweets About Cats And Dogs ...     COMEDY   \n",
       "3  The Funniest Tweets From Parents This Week (Se...  PARENTING   \n",
       "4  Woman Who Called Cops On Black Bird-Watcher Lo...  U.S. NEWS   \n",
       "\n",
       "                                   short_description               authors  \\\n",
       "0  Health experts said it is too early to predict...  Carla K. Johnson, AP   \n",
       "1  He was subdued by passengers and crew when he ...        Mary Papenfuss   \n",
       "2  \"Until you have a dog you don't understand wha...         Elyse Wanshel   \n",
       "3  \"Accidentally put grown-up toothpaste on my to...      Caroline Bologna   \n",
       "4  Amy Cooper accused investment firm Franklin Te...        Nina Golgowski   \n",
       "\n",
       "        date  \n",
       "0 2022-09-23  \n",
       "1 2022-09-23  \n",
       "2 2022-09-23  \n",
       "3 2022-09-23  \n",
       "4 2022-09-22  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique categories: 42\n",
      "['U.S. NEWS' 'COMEDY' 'PARENTING' 'WORLD NEWS' 'CULTURE & ARTS' 'TECH'\n",
      " 'SPORTS' 'ENTERTAINMENT' 'POLITICS' 'WEIRD NEWS' 'ENVIRONMENT'\n",
      " 'EDUCATION' 'CRIME' 'SCIENCE' 'WELLNESS' 'BUSINESS' 'STYLE & BEAUTY'\n",
      " 'FOOD & DRINK' 'MEDIA' 'QUEER VOICES' 'HOME & LIVING' 'WOMEN'\n",
      " 'BLACK VOICES' 'TRAVEL' 'MONEY' 'RELIGION' 'LATINO VOICES' 'IMPACT'\n",
      " 'WEDDINGS' 'COLLEGE' 'PARENTS' 'ARTS & CULTURE' 'STYLE' 'GREEN' 'TASTE'\n",
      " 'HEALTHY LIVING' 'THE WORLDPOST' 'GOOD NEWS' 'WORLDPOST' 'FIFTY' 'ARTS'\n",
      " 'DIVORCE']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "POLITICS          35602\n",
       "WELLNESS          17945\n",
       "ENTERTAINMENT     17362\n",
       "TRAVEL             9900\n",
       "STYLE & BEAUTY     9814\n",
       "PARENTING          8791\n",
       "HEALTHY LIVING     6694\n",
       "QUEER VOICES       6347\n",
       "FOOD & DRINK       6340\n",
       "BUSINESS           5992\n",
       "COMEDY             5400\n",
       "SPORTS             5077\n",
       "BLACK VOICES       4583\n",
       "HOME & LIVING      4320\n",
       "PARENTS            3955\n",
       "THE WORLDPOST      3664\n",
       "WEDDINGS           3653\n",
       "WOMEN              3572\n",
       "CRIME              3562\n",
       "IMPACT             3484\n",
       "DIVORCE            3426\n",
       "WORLD NEWS         3299\n",
       "MEDIA              2944\n",
       "WEIRD NEWS         2777\n",
       "GREEN              2622\n",
       "WORLDPOST          2579\n",
       "RELIGION           2577\n",
       "STYLE              2254\n",
       "SCIENCE            2206\n",
       "TECH               2104\n",
       "TASTE              2096\n",
       "MONEY              1756\n",
       "ARTS               1509\n",
       "ENVIRONMENT        1444\n",
       "FIFTY              1401\n",
       "GOOD NEWS          1398\n",
       "U.S. NEWS          1377\n",
       "ARTS & CULTURE     1339\n",
       "COLLEGE            1144\n",
       "LATINO VOICES      1130\n",
       "CULTURE & ARTS     1074\n",
       "EDUCATION          1014\n",
       "Name: category, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Unique categories:\",df['category'].nunique())\n",
    "print(df['category'].unique())\n",
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 209527 entries, 0 to 209526\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count   Dtype         \n",
      "---  ------             --------------   -----         \n",
      " 0   link               209527 non-null  object        \n",
      " 1   headline           209527 non-null  object        \n",
      " 2   category           209527 non-null  object        \n",
      " 3   short_description  209527 non-null  object        \n",
      " 4   authors            209527 non-null  object        \n",
      " 5   date               209527 non-null  datetime64[ns]\n",
      "dtypes: datetime64[ns](1), object(5)\n",
      "memory usage: 9.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r4/yypyc7lj1316pl97yw3lch680000gn/T/ipykernel_17523/3627053830.py:1: FutureWarning: Treating datetime data as categorical rather than numeric in `.describe` is deprecated and will be removed in a future version of pandas. Specify `datetime_is_numeric=True` to silence this warning and adopt the future behavior now.\n",
      "  df.describe()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>headline</th>\n",
       "      <th>category</th>\n",
       "      <th>short_description</th>\n",
       "      <th>authors</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>209527</td>\n",
       "      <td>209527</td>\n",
       "      <td>209527</td>\n",
       "      <td>209527</td>\n",
       "      <td>209527</td>\n",
       "      <td>209527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>209486</td>\n",
       "      <td>207996</td>\n",
       "      <td>42</td>\n",
       "      <td>187022</td>\n",
       "      <td>29169</td>\n",
       "      <td>3890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>https://www.huffingtonpost.comhttps://www.wash...</td>\n",
       "      <td>Sunday Roundup</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2014-03-25 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "      <td>90</td>\n",
       "      <td>35602</td>\n",
       "      <td>19712</td>\n",
       "      <td>37418</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-01-28 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-09-23 00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     link        headline  \\\n",
       "count                                              209527          209527   \n",
       "unique                                             209486          207996   \n",
       "top     https://www.huffingtonpost.comhttps://www.wash...  Sunday Roundup   \n",
       "freq                                                    2              90   \n",
       "first                                                 NaN             NaN   \n",
       "last                                                  NaN             NaN   \n",
       "\n",
       "        category short_description authors                 date  \n",
       "count     209527            209527  209527               209527  \n",
       "unique        42            187022   29169                 3890  \n",
       "top     POLITICS                            2014-03-25 00:00:00  \n",
       "freq       35602             19712   37418                  100  \n",
       "first        NaN               NaN     NaN  2012-01-28 00:00:00  \n",
       "last         NaN               NaN     NaN  2022-09-23 00:00:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "link                 0\n",
       "headline             0\n",
       "category             0\n",
       "short_description    0\n",
       "authors              0\n",
       "date                 0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Count:     6\n",
      "Name: headline, dtype: int64 \n",
      "Empty Count:     19712\n",
      "Name: short_description, dtype: int64\n",
      "Empty Count: Series([], Name: category, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# checking for empty headlines, description and categories\n",
    "empty_headlines = df[df['headline'] == '']['headline'].value_counts()\n",
    "empty_description = df[df['short_description'] == '']['short_description'].value_counts()\n",
    "empty_category = df[df['category'] == '']['category'].value_counts()\n",
    "\n",
    "print(f'Empty Count: {empty_headlines} \\n'\n",
    "      f'Empty Count: {empty_description}\\n'\n",
    "      f'Empty Count: {empty_category}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "388\n"
     ]
    }
   ],
   "source": [
    "# Dropping Columns that are not necessary for our analysis\n",
    "# Dropping Rows with empty strings in the headline\n",
    "\n",
    "columns_to_drop = ['authors', 'link', 'date']\n",
    "rows_to_drop = df[(df['headline'] == '') | (df['short_description'] == '') | (df['category'] == '')].index\n",
    "\n",
    "filtered_df = df.drop(columns=columns_to_drop)\n",
    "filtered_df.drop(rows_to_drop, inplace=True)\n",
    "# drop rows with duplicate values for short_description and headline\n",
    "print(filtered_df.duplicated(subset=['short_description','headline']).sum())\n",
    "filtered_df.drop_duplicates(subset=['short_description','headline'],keep='last',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of filtered DataFrame: (189426, 3)\n",
      "Number of rows dropped: 20101 rows out of 209527 dropped\n",
      "Percentage of rows dropped: 9.59%\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of filtered DataFrame: {filtered_df.shape}')\n",
    "print(f'Number of rows dropped: {len(df)-len(filtered_df)} rows out of {len(df)} dropped')\n",
    "print(f'Percentage of rows dropped: {((len(df)-len(filtered_df))/len(df)*100):.2f}%') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to merge 'headline' and 'short_description' with \" : \" separator\n",
    "def merge_columns(row):\n",
    "    if pd.isna(row['short_description']):\n",
    "        return row['headline']\n",
    "    else:\n",
    "        return f\"{row['headline']}: {row['short_description']}\"\n",
    "\n",
    "# Apply the function to create a new column\n",
    "filtered_df['news'] = filtered_df.apply(merge_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Over 4 Million Americans Roll Up Sleeves For O...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>American Airlines Flyer Charged, Banned For Li...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23 Of The Funniest Tweets About Cats And Dogs ...</td>\n",
       "      <td>COMEDY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Funniest Tweets From Parents This Week (Se...</td>\n",
       "      <td>PARENTING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Woman Who Called Cops On Black Bird-Watcher Lo...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                news   category\n",
       "0  Over 4 Million Americans Roll Up Sleeves For O...  U.S. NEWS\n",
       "1  American Airlines Flyer Charged, Banned For Li...  U.S. NEWS\n",
       "2  23 Of The Funniest Tweets About Cats And Dogs ...     COMEDY\n",
       "3  The Funniest Tweets From Parents This Week (Se...  PARENTING\n",
       "4  Woman Who Called Cops On Black Bird-Watcher Lo...  U.S. NEWS"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filtered_df = filtered_df[['news', 'category']]\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine which categories should be dropped, after a lot of consideration and inspection\n",
    "categories_to_drop = [\n",
    "    'IMPACT',\n",
    "    'RELIGION', \n",
    "    'MEDIA',\n",
    "    'SCIENCE', \n",
    "    'CRIME',\n",
    "    'HOME & LIVING', \n",
    "    'WOMEN', \n",
    "    'WEIRD NEWS', \n",
    "    'FIFTY', \n",
    "    'GOOD NEWS', \n",
    "    'ARTS & CULTURE',\n",
    "    'DIVORCE',\n",
    "    'WEDDINGS',\n",
    "    'QUEER VOICES',\n",
    "    'BLACK VOICES',\n",
    "    'LATINO VOICES',\n",
    "    'U.S. NEWS',\n",
    "    'COLLEGE',\n",
    "    'EDUCATION',\n",
    "]\n",
    "\n",
    "# merging remaining categories together if they are similar in content\n",
    "category_mapping1 = {\n",
    "    'POLITICS': 'POLITICS',\n",
    "    'WELLNESS': 'WELLNESS',\n",
    "    'HEALTHY LIVING': 'WELLNESS',\n",
    "    'ENTERTAINMENT': 'ART & ENTERTAINMENT',\n",
    "    'COMEDY': 'ART & ENTERTAINMENT',\n",
    "    'ARTS': 'ART & ENTERTAINMENT',\n",
    "    'CULTURE & ARTS': 'ART & ENTERTAINMENT',\n",
    "    'TRAVEL': 'TRAVEL',\n",
    "    'STYLE & BEAUTY': 'STYLE & BEAUTY',\n",
    "    'STYLE': 'STYLE & BEAUTY',\n",
    "    'PARENTING': 'PARENTING',\n",
    "    'PARENTS': 'PARENTING',\n",
    "    'FOOD & DRINK': 'GASTRONOMY',\n",
    "    'TASTE': 'GASTRONOMY',\n",
    "    'BUSINESS': 'BUSINESS & TECH',\n",
    "    'MONEY': 'BUSINESS & TECH',\n",
    "    'TECH': 'BUSINESS & TECH',\n",
    "    'SPORTS': 'SPORTS',\n",
    "    'THE WORLDPOST': 'WORLD NEWS',\n",
    "    'WORLD NEWS': 'WORLD NEWS',\n",
    "    'WORLDPOST': 'WORLD NEWS',\n",
    "    'GREEN': 'ENVIRONMENT',\n",
    "    'ENVIRONMENT': 'ENVIRONMENT',\n",
    "}\n",
    "\n",
    "filtered_df = filtered_df[~filtered_df['category'].isin(categories_to_drop)]\n",
    "filtered_df['category'] = filtered_df['category'].map(category_mapping1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(143201, 2)\n",
      "Unique categories: 11\n",
      "['ART & ENTERTAINMENT' 'PARENTING' 'WORLD NEWS' 'BUSINESS & TECH' 'SPORTS'\n",
      " 'POLITICS' 'ENVIRONMENT' 'WELLNESS' 'STYLE & BEAUTY' 'GASTRONOMY'\n",
      " 'TRAVEL']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "POLITICS               32425\n",
       "WELLNESS               23202\n",
       "ART & ENTERTAINMENT    21339\n",
       "PARENTING              12278\n",
       "STYLE & BEAUTY         11229\n",
       "TRAVEL                  9418\n",
       "BUSINESS & TECH         8939\n",
       "GASTRONOMY              8271\n",
       "WORLD NEWS              8201\n",
       "SPORTS                  4414\n",
       "ENVIRONMENT             3485\n",
       "Name: category, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(filtered_df.shape)\n",
    "print(\"Unique categories:\",filtered_df['category'].nunique())\n",
    "print(filtered_df['category'].unique())\n",
    "filtered_df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of rows dropped: 31.66%\n"
     ]
    }
   ],
   "source": [
    "print(f'Percentage of rows dropped: {((len(df)-len(filtered_df))/len(df)*100):.2f}%') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting as csv:\n",
    "filtered_df.to_csv('/Users/arthur/Downloads/news.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK (performs worse so use spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = filtered_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     news             category\n",
      "0       23 funniest tweet cat dog week sept 1723 dog d...  ART & ENTERTAINMENT\n",
      "1       funniest tweet parent week sept 1723 accidenta...            PARENTING\n",
      "2       puerto ricans desperate water hurricane fionas...           WORLD NEWS\n",
      "3       new documentary capture complexity child immig...  ART & ENTERTAINMENT\n",
      "4       biden un call russian war affront body charter...           WORLD NEWS\n",
      "...                                                   ...                  ...\n",
      "143196  rim ceo thorsten heins significant plan blackb...      BUSINESS & TECH\n",
      "143197  maria sharapova stunned victoria azarenka aust...               SPORTS\n",
      "143198  giant patriot jet colt among improbable super ...               SPORTS\n",
      "143199  aldon smith arrested 49ers linebacker busted d...               SPORTS\n",
      "143200  dwight howard rip teammate magic loss hornet f...               SPORTS\n",
      "\n",
      "[143201 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# lowercase\n",
    "df['news'] = df['news'].str.lower()\n",
    "\n",
    "# Remove punctuation\n",
    "df['news'] = df['news'].apply(lambda text: re.sub(r'[^\\w\\s]', '', text))\n",
    "\n",
    "# Tokenization\n",
    "df['news'] = df['news'].apply(word_tokenize)\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['news'] = df['news'].apply(lambda text: [word for word in text if word not in stop_words])\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['news'] = df['news'].apply(lambda text: [lemmatizer.lemmatize(word) for word in text])\n",
    "\n",
    "# join tokens back to string format for vectorization\n",
    "df['news'] = df['news'].apply(lambda text: ' '.join(text))\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_count = vectorizer.fit_transform(df['news'])\n",
    "y = df['category']\n",
    "\n",
    "# Using TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df['news'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arthur/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7749729408889354\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "ART & ENTERTAINMENT       0.75      0.78      0.76      4320\n",
      "    BUSINESS & TECH       0.64      0.60      0.62      1705\n",
      "        ENVIRONMENT       0.61      0.49      0.55       707\n",
      "         GASTRONOMY       0.81      0.81      0.81      1643\n",
      "          PARENTING       0.75      0.72      0.74      2479\n",
      "           POLITICS       0.83      0.86      0.84      6602\n",
      "             SPORTS       0.77      0.73      0.75       848\n",
      "     STYLE & BEAUTY       0.85      0.81      0.83      2240\n",
      "             TRAVEL       0.79      0.75      0.77      1897\n",
      "           WELLNESS       0.77      0.80      0.79      4599\n",
      "         WORLD NEWS       0.73      0.68      0.70      1601\n",
      "\n",
      "           accuracy                           0.77     28641\n",
      "          macro avg       0.75      0.73      0.74     28641\n",
      "       weighted avg       0.77      0.77      0.77     28641\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_count, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# train logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# predict labels\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tfidf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arthur/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7847142208721762\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "ART & ENTERTAINMENT       0.72      0.81      0.76      4320\n",
      "    BUSINESS & TECH       0.70      0.59      0.64      1705\n",
      "        ENVIRONMENT       0.70      0.43      0.53       707\n",
      "         GASTRONOMY       0.83      0.80      0.82      1643\n",
      "          PARENTING       0.80      0.73      0.76      2479\n",
      "           POLITICS       0.83      0.88      0.85      6602\n",
      "             SPORTS       0.79      0.65      0.71       848\n",
      "     STYLE & BEAUTY       0.86      0.80      0.83      2240\n",
      "             TRAVEL       0.80      0.76      0.78      1897\n",
      "           WELLNESS       0.76      0.84      0.80      4599\n",
      "         WORLD NEWS       0.77      0.68      0.72      1601\n",
      "\n",
      "           accuracy                           0.78     28641\n",
      "          macro avg       0.78      0.72      0.75     28641\n",
      "       weighted avg       0.78      0.78      0.78     28641\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# train the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# predict labels\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# print metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = filtered_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to preprocess text with spaCy\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text.lower())  # Convert to lowercase and process with spaCy\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# apply preprocessing to df\n",
    "df['news'] = df['news'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     news             category\n",
      "0       23 funniest tweet cat dog week sept 17 23 dog ...  ART & ENTERTAINMENT\n",
      "1       funniest tweet parent week sept 17 23 accident...            PARENTING\n",
      "2       puerto rican desperate water hurricane fiona r...           WORLD NEWS\n",
      "3       new documentary capture complexity child immig...  ART & ENTERTAINMENT\n",
      "4       biden un russian war affront body charter whit...           WORLD NEWS\n",
      "...                                                   ...                  ...\n",
      "143196  rim ceo thorsten hein significant plan blackbe...      BUSINESS & TECH\n",
      "143197  maria sharapova stun victoria azarenka austral...               SPORTS\n",
      "143198  giant patriot jet colt   improbable super bowl...               SPORTS\n",
      "143199  aldon smith arrest 49er linebacker bust dui co...               SPORTS\n",
      "143200  dwight howard rips teammate magic loss hornet ...               SPORTS\n",
      "\n",
      "[143201 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_count = vectorizer.fit_transform(df['news'])\n",
    "y = df['category']\n",
    "\n",
    "# Using TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df['news'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arthur/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7787437589469641\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "ART & ENTERTAINMENT       0.75      0.78      0.77      4320\n",
      "    BUSINESS & TECH       0.64      0.62      0.63      1705\n",
      "        ENVIRONMENT       0.62      0.50      0.55       707\n",
      "         GASTRONOMY       0.80      0.81      0.81      1643\n",
      "          PARENTING       0.76      0.72      0.74      2479\n",
      "           POLITICS       0.83      0.86      0.85      6602\n",
      "             SPORTS       0.76      0.72      0.74       848\n",
      "     STYLE & BEAUTY       0.85      0.81      0.83      2240\n",
      "             TRAVEL       0.79      0.75      0.77      1897\n",
      "           WELLNESS       0.77      0.81      0.79      4599\n",
      "         WORLD NEWS       0.74      0.69      0.72      1601\n",
      "\n",
      "           accuracy                           0.78     28641\n",
      "          macro avg       0.76      0.74      0.75     28641\n",
      "       weighted avg       0.78      0.78      0.78     28641\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_count, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tfidf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7881358891100171\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "ART & ENTERTAINMENT       0.74      0.80      0.77      4320\n",
      "    BUSINESS & TECH       0.70      0.61      0.65      1705\n",
      "        ENVIRONMENT       0.68      0.44      0.54       707\n",
      "         GASTRONOMY       0.82      0.82      0.82      1643\n",
      "          PARENTING       0.79      0.72      0.75      2479\n",
      "           POLITICS       0.83      0.88      0.86      6602\n",
      "             SPORTS       0.79      0.67      0.73       848\n",
      "     STYLE & BEAUTY       0.87      0.81      0.83      2240\n",
      "             TRAVEL       0.80      0.77      0.78      1897\n",
      "           WELLNESS       0.76      0.85      0.80      4599\n",
      "         WORLD NEWS       0.78      0.69      0.73      1601\n",
      "\n",
      "           accuracy                           0.79     28641\n",
      "          macro avg       0.78      0.73      0.75     28641\n",
      "       weighted avg       0.79      0.79      0.79     28641\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the logistic regression model\n",
    "model = LogisticRegression(max_iter=10000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Labels (categories)\\nlabels = df['category']\\n\\n# Split the data\\nX_train, X_test, y_train, y_test = train_test_split(X_tfidf, labels, test_size=0.2, random_state=42)\\n\\n# Define the logistic regression model\\nmodel = LogisticRegression(max_iter=1000)\\n\\n# Define the hyperparameters grid\\nparam_grid = {\\n    'C': [1],\\n    'penalty': ['l2'],\\n    'solver': ['liblinear', 'lbfgs'],\\n    'max_iter': [100, 500, 1000, 2000]\\n}\\n\\n# Initialize GridSearchCV\\ngrid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', verbose=5, n_jobs = -1)\\n\\n# Fit GridSearchCV\\ngrid_search.fit(X_train, y_train)\\n\\n# Best model\\nbest_model = grid_search.best_estimator_\\n\\n# Predict on the test set with the best model\\ny_pred = best_model.predict(X_test)\\n\\n# Evaluate the model\\naccuracy = accuracy_score(y_test, y_pred)\\nprint(f'Best Parameters: {grid_search.best_params_}')\\nprint(f'Accuracy: {accuracy:.2f}')\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Labels (categories)\n",
    "labels = df['category']\n",
    "\n",
    "# split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the logistic regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# set parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear', 'lbfgs'],\n",
    "    'max_iter': [100, 500, 1000, 2000]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', verbose=5, n_jobs = -1)\n",
    "\n",
    "# start GriedSearch\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# save best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# predict on the test set with the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# print metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Best Parameters: {grid_search.best_params_}')\n",
    "print(f'Accuracy: {accuracy:.2f}')\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes with GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'alpha': 0.1, 'fit_prior': False}\n",
      "Accuracy: 0.7703990782444747\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "ART & ENTERTAINMENT       0.80      0.73      0.76      4320\n",
      "    BUSINESS & TECH       0.61      0.68      0.64      1705\n",
      "        ENVIRONMENT       0.54      0.62      0.57       707\n",
      "         GASTRONOMY       0.76      0.85      0.80      1643\n",
      "          PARENTING       0.68      0.71      0.70      2479\n",
      "           POLITICS       0.88      0.80      0.84      6602\n",
      "             SPORTS       0.71      0.84      0.77       848\n",
      "     STYLE & BEAUTY       0.82      0.82      0.82      2240\n",
      "             TRAVEL       0.73      0.79      0.76      1897\n",
      "           WELLNESS       0.81      0.77      0.79      4599\n",
      "         WORLD NEWS       0.67      0.79      0.73      1601\n",
      "\n",
      "           accuracy                           0.77     28641\n",
      "          macro avg       0.73      0.76      0.74     28641\n",
      "       weighted avg       0.78      0.77      0.77     28641\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set up Multinomila Naive Bayes\n",
    "model = MultinomialNB()\n",
    "\n",
    "# set parameter grid (not much to tune)\n",
    "param_grid = {\n",
    "    'alpha': [0.01, 0.1, 1, 10, 100],\n",
    "    'fit_prior': [True, False],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# start the GridSearch\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# save best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# predict on the test set with the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# print metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Best Parameters: {grid_search.best_params_}')\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "# print the classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
